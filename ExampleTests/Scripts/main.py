"""Main module for AI-driven testing.

This module contains the primary functionality for executing tests generated by AI models.
"""

import argparse
import json
import os
import time

import docker
import requests
from docker.types import DeviceRequest
from tqdm import tqdm

# Configuration
OLLAMA_IMAGE = "ollama/ollama"
OLLAMA_PORT = 11434
OLLAMA_API_URL = f"http://localhost:{OLLAMA_PORT}/api"
OLLAMA_GEN_ENDPOINT = f"{OLLAMA_API_URL}/generate"
OLLAMA_MODELS_VOLUME = os.path.abspath("./ollama-models")
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))


def start_ollama_container():
    """Start and configure the Ollama Docker container.

    Removes any existing Ollama container, pulls the latest image with
    progress tracking, and starts a new container with GPU support.

    Returns:
        docker.Container: The running Ollama container instance

    Raises:
        docker.errors.APIError: If container operations fail
    """
    client = docker.from_env()

    try:
        existing = client.containers.get("ollama")
        print("Removing existing 'ollama' container...")
        existing.remove(force=True)
    except docker.errors.NotFound:
        pass

    print("Pulling Ollama image with progress tracking...")
    progress_bars = {}
    last_status_key = None
    printed_lines = 0

    for line in client.api.pull(OLLAMA_IMAGE, stream=True, decode=True):
        status = line.get("status")
        layer_id = line.get("id")
        progress = line.get("progressDetail", {})

        if layer_id and "current" in progress and "total" in progress:
            current = progress["current"]
            total = progress["total"]
            if layer_id not in progress_bars:
                progress_bars[layer_id] = tqdm(
                    total=total,
                    desc=f"Layer {layer_id[:10]}",
                    unit="B",
                    unit_scale=True,
                    leave=False,
                )
            progress_bars[layer_id].n = current
            progress_bars[layer_id].refresh()

        status_key = f"{layer_id}:{status}" if layer_id else status
        if status and status_key != last_status_key:
            print(f"{layer_id[:10]}: {status}" if layer_id else status)
            last_status_key = status_key
            printed_lines += 1

    for pbar in progress_bars.values():
        pbar.close()

    print("Starting Ollama container...")
    device_request = DeviceRequest(count=-1, capabilities=[["gpu"]])
    container = client.containers.run(
        OLLAMA_IMAGE,
        name="ollama",
        ports={f"{OLLAMA_PORT}/tcp": OLLAMA_PORT},
        volumes={
            OLLAMA_MODELS_VOLUME: {"bind": "/root/.ollama", "mode": "rw"}
        },
        device_requests=[device_request],
        detach=True,
        remove=True,
    )
    # clear_stdout()

    return container


def wait_for_ollama_api(timeout=60):
    """Wait for the Ollama API to become available.

    Attempts to connect to the Ollama API and waits until it responds
    successfully or times out.

    Args:
        timeout (int): Maximum time to wait in seconds

    Returns:
        bool: True if the API became available

    Raises:
        TimeoutError: If the API doesn't become available within the timeout period
    """
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            r = requests.get(f"{OLLAMA_API_URL}/tags")
            if r.status_code == 200:
                return True
        except requests.exceptions.ConnectionError:
            pass
        time.sleep(2)
    raise TimeoutError("Ollama API did not become available in time.")


def pull_model(model):
    """Pull an LLM model from Ollama's repository.

    Downloads the specified model with progress tracking.

    Args:
        model (str): Name of the model to pull (e.g., "gemma3:4b")

    Raises:
        requests.exceptions.RequestException: If the API request fails
    """
    print(f"Pulling model: {model}")
    with requests.post(
        f"{OLLAMA_API_URL}/pull", json={"name": model}, stream=True
    ) as r:
        r.raise_for_status()
        pbar = None
        last_status = None
        total_size = 0
        printed_lines = 0

        for line in r.iter_lines():
            if not line:
                continue
            try:
                data = json.loads(line.decode("utf-8"))

                status = data.get("status", "")
                total = data.get("total", total_size) or total_size
                completed = data.get("completed", 0)

                if total > total_size:
                    total_size = total

                if pbar is None and total_size > 0:
                    pbar = tqdm(
                        total=total_size,
                        unit="B",
                        unit_scale=True,
                        desc="Model Download",
                        leave=False,
                    )

                if pbar and "completed" in data:
                    pbar.n = completed
                    pbar.refresh()

                if status and status != last_status:
                    print(status)
                    printed_lines += 1
                    last_status = status

            except json.JSONDecodeError:
                decoded = line.decode("utf-8")
                if decoded != last_status:
                    print(decoded)
                    printed_lines += 1
                    last_status = decoded

        if pbar:
            pbar.close()
            # clear_stdout()


def send_prompt(prompt, model, output_file, stream=False):
    """Send a prompt to the Ollama API and save the response to a file.

    Args:
        prompt (str): The prompt to send.
        model (str): The model to use.
        output_file (str): The file to save the response.
        stream (bool): Whether to stream the response.
    """
    # Force a system instruction to make output structured as Markdown
    system_message = (
        "You are a code generation assistant. "
        "Your task is to generate unit tests for the given Python class. "
        "The input will be a Python class, and your output should be a file containing the given python code as well as a Testclass to test it. "
        "The test class should be compatible with the unittest framework. "
        "The Answer should contain only the input class and the test class together in a python code block. "
        "The test class should contain a test function for each method in the input class. "
        "The test file should be runnable and should not contain any errors."
    )

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": stream,
        "system": system_message,
        "options": {
            "seed": 42,
        },
    }

    if stream:
        collected_response = ""
        with requests.post(
            OLLAMA_GEN_ENDPOINT, json=payload, stream=True
        ) as r:
            r.raise_for_status()
            for chunk in r.iter_lines():
                if chunk:
                    decoded_chunk = chunk.decode("utf-8")
                    try:
                        chunk_json = json.loads(decoded_chunk)
                        chunk_response = chunk_json.get("response", "")
                        # print(chunk_response, end="", flush=True)
                        collected_response += chunk_response
                    except json.JSONDecodeError:
                        continue

        with open(output_file, "w", encoding="utf-8") as f:
            f.write(collected_response)

        return collected_response

    else:
        r = requests.post(OLLAMA_GEN_ENDPOINT, json=payload)
        r.raise_for_status()
        response_text = r.json().get("response", "")

        with open(output_file, "w", encoding="utf-8") as f:
            f.write(response_text)

        return response_text


def clear_stdout():
    """Clear the terminal screen.

    Uses the appropriate system command based on the operating system.
    """
    if os.name == "nt":
        os.system("cls")
    else:
        os.system("clear")


def read_prompt(file):
    """Read prompt content from a file.

    Args:
        file (str): Path to the prompt file

    Returns:
        str: Content of the prompt file

    Raises:
        FileNotFoundError: If the specified file doesn't exist
        IOError: If there's an error reading the file
    """
    with open(file, "r") as f:
        return f.read()


if __name__ == "__main__":
    # Setup command line arguments
    parser = argparse.ArgumentParser(
        description="Run Ollama prompt sending script."
    )
    parser.add_argument(
        "--prompt_file",
        type=str,
        default=os.path.join(SCRIPT_DIR, "prompt.txt"),
        help="Path to the input prompt file (default: prompt.txt in the same directory)",
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default=os.path.join(SCRIPT_DIR, "output.py"),
        help="Path to save the output (default: output.md in the same directory)",
    )
    parser.add_argument(
        "--llm_name",
        type=str,
        required=True,
        help="Name of the LLM to use (e.g., gemma3:4b)",
    )

    args = parser.parse_args()

    MODEL_NAME = args.llm_name

    container = start_ollama_container()
    try:
        print("Waiting for Ollama API...")
        wait_for_ollama_api()

        pull_model(MODEL_NAME)
        # clear_stdout()

        print("Sending prompt...")
        prompt = read_prompt(args.prompt_file)
        # print(f"Prompt: \n{prompt}")
        # print("Answer:")
        response = send_prompt(
            prompt=prompt,
            model=MODEL_NAME,
            stream=True,
            output_file=args.output_file,
        )

    finally:
        print("Stopping container...")
        container.stop()
